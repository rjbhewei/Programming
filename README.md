# Heracles: Improving Resource Efficiency at Scale

### 摘要

类似于 *websearch* 这种面向用户、延迟敏感的服务, 在日常低流量的情况的下未能充分的利用计算资源，很少在生产性服务商为其他的任务重新利用这些资源，因为争夺共享资源会让延迟导致毛刺，以致违背了延迟敏感敏感人物的 *SLO* (service-level objectives)

### 介绍

公有云和私有云框架允许我们持续增长成千上万台服务器的大型数据中心的负载数量，云服务的商业模式强调降低基础架构成本,,因而需要继续扩大服务器的最大化利用率

### 共享资源的干扰

当两个或者多个工作负载同时在一台服务器上执行的时候，它们会竞争共享资源，这个章节我们回顾下主要的干扰源、可用的隔离机制和动态管理的动机

在一台服务器上最主要的共享资源是在一个或者多个cpu卡槽上的核心，我们不能简单的使用cgroups cpuset机制静态绑定核心给LC服务和BE任务混布的场景，当类似搜索这种面向用户的服务会出现负载毛刺，它们需要所有可用的核区面对吞吐的需求而不是导致违背SLO，同样我们不能简单的分配高优先级给LC服务和依靠操作系统的调度任务对核心的分配，通常的调度算法如linux的CFS算法，有漏洞，当LC服务和BE任务混布的时候以至于频繁的导致LC服务违背SLO，实时调度算法(eg:FIFO),会工作不饱和导致很低的利用率，
在英特尔核心上可用的超线程技术导致进一步的并发症，例如超线程执行BE任务能干扰LC超线程指令带宽、共享L1和L2级缓存，和TLBs

大量研究表明，在共享资源中的不可控的最后一级缓存干扰(LLC)对混部是有很大影响的。为了解决这个问题，英特尔最近介绍在服务器芯片上LLC缓存分区，这个功能叫做缓存分配技术(CAT),

1. 内存带宽


	大部分LC服务都是操作着很大的数据集，这个导致不能够CPU的CACHE是存放不下的,因而，在高负载的时候频繁的访问内存，使得内存带宽压力增大，以致于会对内存带宽比较敏感，尽管有了很多对内存带宽隔离上面的重要研究，但在商用芯片上还没有硬件隔离机制，在一些多socket服务器上，可以通过NUMA管道隔离不同的工作负载，但这种方法约束了内存的容量分配和寻址，内存带宽缺乏硬件支持，导致工作负载动态管理隔离复杂并且效率约束
	
2. 网络带宽

	数据中心负载扩展了应用增加网络流量，很多数据中心采用丰富的拓扑结构来均分网络流量来避免网络汇中路由拥塞现象，这里有多种网络协议，用来优先处理LC服务的短消息再处理BE任务的大消息，在一个服务器内，拥塞问题可能发生在网络入口也可能发生在网络出口，当一个BE任务导致网络拥塞，我们可以调整cpu核心的分配直到网络限流机制触发，在流量出口方向，我们可以用linux操作系统的交通管控(Traffic control)机制来提供对LC服务的网络带宽的保证，使得它们的信息优先于BE任务的信息，网络带块的交通管控必须能够随着系统负载被动态管理，另外静态网络流量优先级，可能会导致资源利用率不高甚至出现饿死现象
	
3. 能耗(power)
	
	在混部场景下,能耗是一个额外的干扰源，所有现代的多核芯片都有一些形式上的动态超频，比如英特尔芯片中的Turbo Boost以及AMD芯片中的Turbo Core，在能耗有余量的情况下这些技术提升了处理器芯片比标称频率的更高的频率，因而，LC任务使用的核心的时钟频率不仅仅取决于它自己的负载而且取决于运行在相同槽位上面的BE任务的强度，换句话说，LC任务可能会因为混部任务导致性能频繁的意外下降，这个干扰可以通过每个核心的动态电压频率缩放来减少( This interference can be mitigated with per-core dynamic voltage frequency scaling)，做为运行BE任务的核可以降低它们的频率使得LC任务保持正常的频率，静态策略会以最低的频率运行所有的BE任务从而确保LC任务没有能耗方面的限制，但是这个方法严重的惩罚了绝大部分的BE任务，大部分BE任务没有配置
	
4. 跨资源干扰
	
	混部的一个严重的挑战是跨资源干扰，一个BE任务可能会引发上面讨论过的所有共享资源的干扰，类似的，很多LC服务都会对多种资源干扰敏感，因而不能能够只去管理一种干扰资源，所有潜在的资源需要被监控和小心的隔离，另外干扰源是彼此互相交互的，例如，LLC的争夺可能会导致两种类型的任务都去需要更多的内存带宽从而可能导致瓶颈，类似的，一个任务注意到了网络拥塞问题从而企图使用数据压缩的方式可能会导致cpu和功耗方面的争夺，理论上，干扰可能的数量会是干扰源数据的平方，导致这成了一个非常困难的问题
	
### 干扰表征和分析	

本章节描述共享资源对延迟敏感服务的干扰特点

1. 延迟敏感的工作负载

	我们使用三个谷歌线上的延迟敏感的服务做为介绍，websearch 是线上web搜索服务中的一个查询服务部分，他是一个有一定规模的工程，通过大型的fan-out架构，提供了严格的延迟SLO且很高的吞吐量，拥有上千个用于查询搜索索引分片的子节点，对于这些叶子节点的SLO 99%的延迟在几十毫秒，websearch 负载的增加使用的是真实的用户查询中的匿名的追踪
	
	websearch 因为会存储很多的索引分片在内存中，因而有很高的内存使用，同时它还有适度的内存带宽需求，在负载100%的时候会占用40%的可用内存带宽，有很大一部分的索引侵入丢失在LLC上，这里有一个小但是重要的指令工作集和数据集在LLC中，它需要计算得分和排序搜索的hits从而有一定的cpu计算，，但是它不需要很大数量的网络带宽，为了这个研究，为了能够混部BE任务，我们使用较小份的内存在搜索服务上

	ml_cluster是一个独立的服务，用机器学习的技术来做实时集群文本分析，很多谷歌应用服务使用ml_cluster给集群分配文本信息的特征/片段，ml_cluster通过以前的离线训练模型把文本存放在集群中，这个模型因为性能的考虑主要存放在内存中，ml_cluster的SLO是 95%的延迟保证在几十毫秒，ml_cluster 采用的是捕获生产服务中真实的匿名请求追踪
	
	对比于websearch, ml_cluster需要更多的内存带宽(在峰值的时候需要60%的内存带宽)，但是只需要很少的cpu计算(整体来看cpu的能耗也是很低的)，同时对网络带宽的需求也是很低的，ml_cluster一个有趣的现象是它的每个请求都有少量的缓存足迹，但是存在很多重要的请求，就会转化成为一个很大数量的缓存压力，从而蔓延到对内存的压力，这个反应到我门的分析中是内存带宽随着负载是一个超线性的增长关系
	
	memkeyval 是一个内存中的key-value的存储服务，类似于 memchached，memkeyval被用于很多谷歌的后端web服务的缓存系统，其他的大型web服务公司如facebook和twiter广泛采用的是memcached，memkeyval明显的减少了每个请求处理耗时，对比于websearch,在峰值需要每秒几十万的请求指令导致它需要极高的吞吐量，因而任何请求的处理耗时必须很快，这SLO 是时延非常的低，99%的延迟在百来微秒的时延，memkeyval负载的增加方式跟ml_cluster一样
	
	在负载峰值，memkeyval会被网络带宽限制，尽管每个请求采用的是很小数量的网络协议，在高的请求率下导致memkeyval成为cpu密集型计算，相比之下，内存带宽的需求要比较小(在最大负载的时候使用20%的内存带宽)，request请求从内存中简单的接受value并且放入response中发送给网络，memkeyval既有对LLC静态工作集指令也有每个请求数据集
	
2. 表征方法

	

	
	
	
	
	
	
	
	
	

	
